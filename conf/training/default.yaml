# Repo/conf/training/default.yaml
# Default training parameters

# Data splitting and loading
val_size: 0.2
batch_size: 128
num_workers: 2 # Adjusted for Colab/general use, was 6/16 in original comments
persistent_workers: true # Was false in original comments

# Training loop
epochs: 100
patience: 20 # Early stopping patience
label_smoothing: 0.1 # Label smoothing factor for CrossEntropyLoss (0.0 = disabled)

# Optimizer settings
optimizer: Adam
lr: 1e-3
weight_decay: 1e-3

# Scheduler settings
scheduler:
  # --- Common ---
  type: StepLR # Options: StepLR, CyclicLR, OneCycleLR, CosineAnnealingLR
  verbose: true

  # --- StepLR specific ---
  step_size: 5 # For StepLR: Period of learning rate decay.
  gamma: 0.8   # For StepLR: Multiplicative factor of learning rate decay.

  # --- CyclicLR specific ---
  # See: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html
  base_lr: 1e-4       # For CyclicLR: Initial learning rate which is the lower boundary in the cycle.
  max_lr: 1e-3        # For CyclicLR: Upper learning rate boundary, which is scaled relative to base_lr.
  step_size_up: 50  # For CyclicLR: Number of training iterations in the increasing half of a cycle.
  # mode: "triangular" # Optional: "triangular", "triangular2", "exp_range". Default: "triangular"
  # gamma: 1.0         # Optional: Constant in 'exp_range' scaling function. Default: 1.0
  # cycle_momentum: True # Optional: If True, momentum is cycled inversely to learning rate between 'base_momentum' and 'max_momentum'. Default: True
  # base_momentum: 0.8   # Optional: Lower momentum boundary. Default: 0.8
  # max_momentum: 0.9    # Optional: Upper momentum boundary. Default: 0.9

  # --- Warmup (Optional, needs implementation in training script if used) ---
  warm_epochs: 10 # Parameter for potential warmup phase (currently not implemented in train_crossval.py)

  # --- OneCycleLR specific ---
  # See: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html
  pct_start: 0.3     # For OneCycleLR: Percentage of the cycle spent increasing the learning rate.

  # --- CosineAnnealingLR specific ---
  # See: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html
  T_max: 100      # For CosineAnnealingLR: Maximum number of iterations. (Often set to total training iterations)
  eta_min: 0      # For CosineAnnealingLR: Minimum learning rate. Default: 0.


# Hardware and display
device_id: 0
disable_bat_pbar: true # Progress bar display
